# LLM Configuration
LLM_BASE_URL=http://localhost:1234/v1
LLM_MODEL=local-model

# Flask Settings
FLASK_HOST=0.0.0.0
FLASK_PORT=5000
FLASK_DEBUG=True

# Workspace
WORKSPACE_DIR=workspace

# AWS Logging
# Set to false to see verbose botocore DEBUG logs (defaults to true/suppressed)
SUPPRESS_AWS_LOGGING=true
# LLM Provider Configuration
# Options: local, openai, anthropic, bedrock
LLM_PROVIDER=local

# Local (LM Studio) - default
LLM_BASE_URL=http://localhost:1234/v1
LLM_MODEL=local-model

# OpenAI (set LLM_PROVIDER=openai)
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o
# OPENAI_API_KEY=sk-...

# Anthropic (set LLM_PROVIDER=anthropic)
# LLM_MODEL=claude-sonnet-4-20250514
# ANTHROPIC_API_KEY=sk-ant-...

# AWS Bedrock (set LLM_PROVIDER=bedrock)
# check available models with:
# 'aws bedrock list-foundation-models --region us-west-1 --query "modelSummaries[?contains(modelId, 'claude')].modelId" --profile BedrockDeveloper-572929969205'
# LLM_MODEL=us.anthropic.claude-3-5-sonnet-20241022-v2:0
# AWS_REGION=us-west-1
#
# Option 1: Use SSO profile (recommended for IAM Identity Center)
# AWS_PROFILE=your-sso-profile
#
# Option 2: Temporary credentials from SSO portal (https://usr.awsapps.com/start/#)
# AWS_ACCESS_KEY_ID=ASIA...
# AWS_SECRET_ACCESS_KEY=...
# AWS_SESSION_TOKEN=...
#
# Option 3: IAM user credentials (no session token needed)
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=...
