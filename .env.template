# LLM Provider Configuration
# Options: local, openai, anthropic, bedrock
LLM_PROVIDER=local

# Local (LM Studio) - default
LLM_BASE_URL=http://localhost:1234/v1
LLM_MODEL=local-model

# OpenAI (set LLM_PROVIDER=openai)
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o
# OPENAI_API_KEY=sk-...

# Anthropic (set LLM_PROVIDER=anthropic)
# LLM_MODEL=claude-sonnet-4-20250514
# ANTHROPIC_API_KEY=sk-ant-...

# AWS Bedrock (set LLM_PROVIDER=bedrock)
# LLM_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
# AWS_REGION=us-east-1
# Note: AWS credentials come from ~/.aws/credentials or IAM role
